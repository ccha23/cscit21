{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb70a32d-8dc5-4b99-bcdf-15f6adb58d15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mutual Information Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c1b3c-d6cd-47d8-9d06-778147e8650a",
   "metadata": {},
   "source": [
    "$\\def\\abs#1{\\left\\lvert #1 \\right\\rvert}\n",
    "\\def\\Set#1{\\left\\{ #1 \\right\\}}\n",
    "\\def\\mc#1{\\mathcal{#1}}\n",
    "\\def\\M#1{\\boldsymbol{#1}}\n",
    "\\def\\R#1{\\mathsf{#1}}\n",
    "\\def\\RM#1{\\boldsymbol{\\mathsf{#1}}}\n",
    "\\def\\op#1{\\operatorname{#1}}\n",
    "\\def\\E{\\op{E}}\n",
    "\\def\\d{\\mathrm{\\mathstrut d}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9909a95a-8dcf-42e8-a826-3f7819c4ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dv import *\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import tensorboard as tb\n",
    "\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5122355-6814-4dc9-aee0-41cc985dd99a",
   "metadata": {},
   "source": [
    "**How to estimate MI via KL divergence?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff0ced-ebed-4598-ae4e-2d67ea8feaca",
   "metadata": {},
   "source": [
    "In this notebook, we will introduce a few methods of estimating the mutual information via KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ae125-e9ab-4c63-a72b-a5003f916656",
   "metadata": {},
   "source": [
    "We first introduce the Mutual Information Neural Estimation (MINE) method in {cite}`belghazi2018mine`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54672af4-c076-476b-ac43-070c4af7b5e1",
   "metadata": {},
   "source": [
    "## MINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a638b-8984-4036-9d15-621f9dbf89cf",
   "metadata": {},
   "source": [
    "The idea is to obtain MI {eq}`MI` from KL divergence {eq}`D` as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be906a06-a763-410b-9098-ac57f9d918ca",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "I(\\R{X}\\wedge \\R{Y}) = D(\\underbrace{P_{\\R{X},\\R{Y}}}_{P_{\\R{Z}}}\\| \\underbrace{P_{\\R{X}}\\times P_{\\R{Y}}}_{P_{\\R{Z}'}}).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db7e9cf-28b2-4919-9c5c-2c8e7a68056e",
   "metadata": {},
   "source": [
    "and then apply the DV formula {eq}`avg-DV` to estimate the divergence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c1971-d508-42d4-939e-9ec3a9981ce3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Definition** MINE  \n",
    ":label: MINE\n",
    "\n",
    "The mutual information neural estimation (MINE) of $I(\\R{X}\\wedge\\R{Y})$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\R{I}_{\\text{MINE}} := \\sup_{t_{\\theta}: \\mc{Z} \\to \\mathbb{R}} \\overbrace{\\frac1n \\sum_{i\\in [n]} t_{\\theta}(\\R{X}_i,\\R{Y}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\theta}(\\R{X}'_i,\\R{Y}'_i)}}^{-\\R{L}_{\\text{MINE}}(\\theta):=}\n",
    "\\end{align}\n",
    "$$ (MINE)\n",
    "\n",
    "where \n",
    "\n",
    "- the supremum is over $t_{\\theta}$ representable by a neural network with trainable/optimizable parameters $\\theta$,\n",
    "- $P_{\\R{X}',\\R{Y}'}:=P_{\\R{X}}\\times P_{\\R{Y}}$, and\n",
    "- $(\\R{X}'_i,\\R{Y}'_i\\mid i\\in [n'])$ is the sequence of i.i.d. samples of $P_{\\R{X}',\\R{Y}'}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00017f18-d719-41f2-aa8e-53ff24f5e8e6",
   "metadata": {},
   "source": [
    "The above actually does not completely define MINE. There are some implementation details to be filled in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeba77f-0a85-4066-9696-90a73634d56f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**How to obtain the reference samples ${\\R{Z}'}^{n'}$, i.e., ${\\R{X}'}^{n'}$ and ${\\R{Y}'}^{n'}$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61052a26-0343-4e24-b18a-3899ac0617dc",
   "metadata": {},
   "source": [
    "We can approximate the i.i.d. sampling of $P_{\\R{X}}\\times P_{\\R{Y}}$ using samples from $P_{\\R{X},\\R{Y}}$ by a re-sampling trick:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c3b0c0-7b33-4a32-a426-21a14a323a69",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "P_{\\R{Z}'^{n'}} &\\approx P_{((\\R{X}_{\\R{J}_i},\\R{Y}_{\\R{K}_i})\\mid i \\in [n'])}\n",
    "\\end{align}\n",
    "$$ (resample)\n",
    "\n",
    "where $\\R{J}_i$ and $\\R{K}_i$ for $i\\in [n']$ are independent and uniformly random indices\n",
    "\n",
    "$$\n",
    "P_{\\R{J},\\R{K}} = \\op{Uniform}_{[n]\\times [n]}\n",
    "$$\n",
    "\n",
    "and $[n]:=\\Set{1,\\dots,n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c465a-2b21-42ec-843a-6715c2ba6339",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "MINE {cite}`belghazi2018mine` uses the following implementation that samples $(\\R{J},\\R{K})$ but without replacement. You can change $n'$ using the slider for `n_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b5202-d666-4e3d-a861-0bd2358fb26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "\n",
    "def resample(data, size, replace=False):\n",
    "    index = rng.choice(range(data.shape[0]), size=size, replace=replace)\n",
    "    return data[index]\n",
    "\n",
    "\n",
    "@widgets.interact\n",
    "def plot_resampled_data_without_replacement(n_=(2, n)):\n",
    "    XY_ = np.block([resample(XY[:, [0]], n_), resample(XY[:, [1]], n_)])\n",
    "    resampled_data = pd.DataFrame(XY_, columns=[\"X'\", \"Y'\"])\n",
    "    p_ = plot_samples_with_kde(resampled_data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6eba5e-d6e9-48fc-956d-694c470b6e5e",
   "metadata": {},
   "source": [
    "In the above, we defined the function `resample` that \n",
    "- uses `choice` to uniformly randomly select a choice\n",
    "- from a `range` of integers going from `0` to \n",
    "- the size of the first dimension of the `data`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75febee6-6e07-4b1b-8636-d88d241fdb56",
   "metadata": {},
   "source": [
    "Note however that the sampling is without replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22285f9a-13a4-4fc0-8d48-b3ab8005f088",
   "metadata": {},
   "source": [
    "**Is it a good idea to sample without replacement?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710fac34-6635-4f55-b9b1-47e91fa017d1",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Sampling without replacement has an important restriction $n'\\leq n$. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247f302-7e15-4182-b6a9-7b8868cabf12",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77205214fa4e52f5d61d3fc48938d5b0",
     "grade": true,
     "grade_id": "without-replacement",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3015b30c-0fd3-4e37-a3bd-7bdf389ed9e1",
   "metadata": {},
   "source": [
    "**Exercise** To allow $n>n'$, complete the following code to sample with replacement and observe what happens when $n \\gg n'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbcc28-c074-431b-8def-77d1b573b5d6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a813a218b6d1e3e3103b01bde9eaae4",
     "grade": false,
     "grade_id": "resample",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@widgets.interact\n",
    "def plot_resampled_data_with_replacement(\n",
    "    n_=widgets.IntSlider(n, 2, 10 * n, continuous_update=False)\n",
    "):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    resampled_data = pd.DataFrame(XY_, columns=[\"X'\", \"Y'\"])\n",
    "    p_ = plot_samples_with_kde(resampled_data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abcd98e-d07b-426b-9724-52a16e76edf2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Exercise** \n",
    "\n",
    "Explain whether the resampling trick gives i.i.d. samples $(\\R{X}_{\\R{J}_i},\\R{Y}_{\\R{K}_i})$ for the cases with replacement and without replacement respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b046f-629e-4151-ba09-7dac76de8b7d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5d1dc766bd8ce4ba5b7e66b83f1e039",
     "grade": true,
     "grade_id": "non-iid",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963bca6-3230-484e-bca6-15dac88f6352",
   "metadata": {},
   "source": [
    "To improve the stability of the training, MINE applies additional smoothing to the gradient calculation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa76d5-a4cb-4d80-a37f-ef527d537112",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\R{L}_{\\text{MINE}}(\\theta) &= \\overbrace{- \\frac{1}{n} \\sum_{i\\in [n]}  t_{\\theta} (\\R{X}_i, \\R{Y}_i) }^{\\R{L}_1(\\theta):=} + \\log \\overbrace{\\frac{1}{n'} \\sum_{i\\in [n']}  e^{t_{\\theta} (\\R{X}'_i, \\R{Y}'_i)}}^{\\R{L}_2(\\theta):=}\\\\\n",
    "\\nabla \\R{L}_{\\text{MINE}}(\\theta) &= \\nabla \\R{L}_1(\\theta) + \\frac{\\nabla \\R{L}_2(\\theta)}{\\R{L}_2(\\theta)}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c8129-33d1-4127-a40c-0592cecb1368",
   "metadata": {},
   "source": [
    "Variation in $\\nabla \\R{L}_2(\\theta)$ leads to the variation of the overall gradient especially when $\\R{L}_2(\\theta)$ is small. With minibatch gradient descent, the sample average is over a small batch and so the variance can be quite large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6c0a3-b9de-47ff-b2ab-27d7b56fe40b",
   "metadata": {},
   "source": [
    "To alleviate such variation, MINE replaces the denominator $\\R{L}_2(\\theta)$ by its moving average:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b409c4-4eab-457b-abf3-1eace29740a0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta^{(i+1)} := \\theta^{(i)} - s^{(i)} \\nabla \\R{L}_1 (\\theta^{(i)}) + \\frac{\\nabla \\R{L}_2(\\theta^{(i)})}{\\overline{\\R{L}}_2^{(i)}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\overline{\\R{L}}_2^{(i)} =  \\beta \\overline{\\R{L}}_2^{(i-1)} + (1-\\beta) \\R{L}_2(\\theta^{(i)})\n",
    "$$\n",
    "\n",
    "for some smoothing factor $\\beta\\in [0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9135e-85d5-4bf8-b33f-00802377d76c",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Implement a neural network trainer for MINE similar to `DVTrainer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9f8ae-93e6-479b-be48-081e66ab6413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DVTrainer??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad56f6dd-e301-4c0e-acd2-ed9c17f79d20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MI-NEE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522dede-66d5-44ad-b508-3ac675793d4c",
   "metadata": {},
   "source": [
    "**Is it possible to generate i.i.d. samples for ${\\R{Z}'}^{n'}$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584a25f-755d-4e98-8307-5cf3c37b7654",
   "metadata": {},
   "source": [
    "Consider another formula for mutual information:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc81b16-5238-477c-80b2-48b2c528f4bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Proposition**  \n",
    ":label: MI-3D\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &= D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'}) - D(P_{\\R{X}}\\|P_{\\R{X}'}) - D(P_{\\R{Y}}\\|P_{\\R{Y}'})\n",
    "\\end{align}\n",
    "$$ (MI-3D)\n",
    "\n",
    "for any product reference distribution $P_{\\R{X}'}\\times P_{\\R{Y}'}$ for which the divergences are finite.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc1af9-d7e8-49f3-9509-ed77a7429f75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Corollary**  \n",
    ":label: MI-ub\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &= \\inf_{\\substack{P_{\\R{X}'}\\in \\mc{P}(\\mc{X})\\\\ P_{\\R{Y}'}\\in \\mc{P}(\\mc{Y})}} D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'}).\n",
    "\\end{align}\n",
    "$$ (MI-ub)\n",
    "\n",
    "where the optimal solution is $P_{\\R{X}'}\\times P_{\\R{Y}'}=P_{\\R{X}}\\times P_{\\R{Y}}$, the product of marginal distributions of $\\R{X}$ and $\\R{Y}$. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf194fa-fa9a-4d17-a70f-213c07e4b1f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Proof**\n",
    "\n",
    "{eq}`MI-ub` follows from {eq}`MI-3D` directly since the divergences are non-negative. To prove the proposition:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &= H(\\R{X}) + H(\\R{Y}) - H(\\R{X},\\R{Y})\\\\\n",
    "&= E\\left[-\\log dP_{\\R{X}'}(\\R{X})\\right] - D(P_{\\R{X}}\\|P_{\\R{X}'})\\\\\n",
    "&\\quad+E\\left[-\\log dP_{\\R{Y}'}(\\R{Y})\\right] - D(P_{\\R{Y}}\\|P_{\\R{Y}'})\\\\\n",
    "&\\quad-E\\left[-\\log d(P_{\\R{X}'}\\times P_{\\R{Y}'})(\\R{X},\\R{Y})\\right] + D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'})\\\\\n",
    "&= D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'}) - D(P_{\\R{X}}\\|P_{\\R{X}'}) - D(P_{\\R{Y}}\\|P_{\\R{Y}'})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1fecbc-9c51-47c6-b88b-7e1a5adeee5f",
   "metadata": {},
   "source": [
    "*Mutual Information Neural Entropic Estimation (MI-NEE)* {cite}`chan2019neural` uses {eq}`MI-3D` to estimate MI by estimating the three divergences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c956612-7bea-4ddc-a0fc-e36edd3fc439",
   "metadata": {},
   "source": [
    "Applying {eq}`avg-DV` to each divergence in {eq}`MI-3D`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fa4fc-f584-4bda-b570-2e0d5aa1b590",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &\\approx \\sup_{t: \\mc{Z} \\to \\mathbb{R}} \\frac1n \\sum_{i\\in [n]} t_{\\R{X},\\R{Y}}(\\R{X}_i,\\R{Y}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\R{X},\\R{Y}}(\\R{X}'_i,\\R{Y}'_i)}\\\\\n",
    "&\\quad - \\sup_{t: \\mc{Z} \\to \\mathbb{R}} \\frac1n \\sum_{i\\in [n]} t_{\\R{X}}(\\R{X}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\R{X}}(\\R{X}'_i)} \\\\\n",
    "&\\quad - \\sup_{t: \\mc{Z} \\to \\mathbb{R}} \\frac1n \\sum_{i\\in [n]} t_{\\R{Y}}(\\R{Y}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\R{Y}}(\\R{Y}'_i)}\n",
    "\\end{align}\n",
    "$$ (MI-NEE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7eb69-7474-4e14-9cfb-d4faea394d7c",
   "metadata": {},
   "source": [
    "$P_{\\R{X}'}$ and $P_{\\R{Y}'}$ are known distributions and so arbitrarily many i.i.d. samples can be drawn from them directly without using the resampling trick {eq}`resample`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4576dba-9848-4689-b84e-59087d83ccd4",
   "metadata": {},
   "source": [
    "Indeed, since the choice of $P_{\\R{X}'}$ and $P_{\\R{Y}'}$ are arbitrary, we can also also train neural networks to optimize them. In particular, {eq}`MI-ub` is a special case where we can train neural networks to approximate $P_{\\R{X}}$ and $P_{\\R{Y}}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
